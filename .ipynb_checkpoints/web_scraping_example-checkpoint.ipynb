{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66336c4a-1e67-450c-a60e-2e43b3950c74",
   "metadata": {},
   "source": [
    "# Web Scraping Example\n",
    "# Fall 2021\n",
    "### Drake Wagner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0811c1-7c98-48ab-a071-ce9886b79f79",
   "metadata": {},
   "source": [
    "#### This example can be found at: https://github.com/DrakeWagner/ds-5100-web-scraping-examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7463e8-885e-4053-a437-1e3a610a21db",
   "metadata": {},
   "source": [
    "Beautiful Soup Docs: https://beautiful-soup-4.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f39ed4-ea48-4664-8955-a956de4d680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080b133b-3662-4852-946d-33829343b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-agent': 'UVA Class Example (dbw2tn@virginia.edu) (Language=Python 3.8.2; Platform=Linux(MX-19.4 / 31))'} \n",
    "# this \"header\" is sent along with the request to the website. Optional, but good habit so the owner of the site can see who is visiting.\n",
    "# more info on headers: https://docs.developer.amazonservices.com/en_US/dev_guide/DG_UserAgentHeader.html\n",
    "\n",
    "URL = 'http://books.toscrape.com/'\n",
    "\n",
    "page = requests.get(URL, headers = headers) # request html content from the site\n",
    "page # checks if access was granted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6637da-4104-47ae-8cdd-8335e193a418",
   "metadata": {},
   "source": [
    "Status codes for web requests: https://www.w3.org/Protocols/HTTP/HTRESP.html\n",
    "For example, if you were to get `<Response [404]>` from the cell above, this would indicate that no server was found from the given URL. `Response [200]` means we have access to the html content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe819b4-b3e6-47c8-91f9-126a3e443f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "# This line creates an object that takes the HTML content that we scraped with requests.get() as the input\n",
    "# The html.parser tells our webscraper that we are working with html data\n",
    "# There are other parsers for other types of documents, such as xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3769a79a-b97a-4f65-a3f8-6fb602531c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# page.text     # shows us the source html, if run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326f1685-c429-444b-a675-5e9638697ebb",
   "metadata": {},
   "source": [
    "Now that we have our html data scraped and assigned to a variable, it's time to start digging through and looking for the information we want to extract.\n",
    "\n",
    "A line of html code contains:\n",
    "1. A tag\n",
    "2. an attribute\n",
    "3. a value\n",
    "4. a string/text\n",
    "\n",
    "This link gives a good overview of the parts of html, as well as further tips for working with Beautiful Soup: https://www.pluralsight.com/guides/web-scraping-with-beautiful-soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aef0e6-bd4a-4c3c-b5b3-482b5294d917",
   "metadata": {},
   "source": [
    "Going to your website and pressing `F12` opens up devtools, which lets us see the html of the website in hierarchical structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893c084e-1531-4802-8082-cb511b9fad19",
   "metadata": {},
   "source": [
    "Let's make a list of all the books on the first page. Start by inspecting the html code. We see that the title of each book is displayed under a \"h3\" tag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4b2e6d-7556-461a-9920-4f4015337b17",
   "metadata": {},
   "source": [
    "![alt text](book_ss1.png \"webpage\")\n",
    "![alt text](book_ss2.png \"html code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d2c4a1-a572-4e2c-915e-4b94de5b839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_list = []\n",
    "\n",
    "books = soup.find_all('h3') # searches the html for all instances with a 'h3' tag\n",
    "                            # inspecting the webpage shows us that this specific tag hosts each book title\n",
    "for i in books:\n",
    "    book_list.append(i.get_text()) # get_text() removes the html tags/junk and keeps just the text\n",
    "                                   # here, we loop through for each instance of 'h3' and extract just the text\n",
    "\n",
    "book_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014febf3-75d8-4aee-abdf-d081c0e7ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do another example where we find the genres from the list on the left of the website\n",
    "\n",
    "genres = soup.find('ul', {'class' : 'nav'}) # the webpage shows each tag with a genre also has the class 'nav'\n",
    "                                            # this can also be written as soup.find('ul', class_='nav')\n",
    "genres = str(genres.get_text()) # remove html tags/etc.\n",
    "genres = genres.replace(' ', ' ').replace('\\n', '').split() # get rid of whitespace/new lines\n",
    "\n",
    "genres[0:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
